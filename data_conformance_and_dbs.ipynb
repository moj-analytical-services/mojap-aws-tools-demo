{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Conformance and DBs\n",
    "\n",
    "This tutorial demonstrates how databases can be created with different table schemas (mainly different CSV types, JSON types and parquet types).\n",
    "\n",
    ">_Notes this is a fairly deep dive into the GlueConverter as well._\n",
    "\n",
    "It also demonstrates how to ensure metadata from data from external sources can be maintained between file in S3 -> Arrow/Pandas -> Glue Database.\n",
    "\n",
    "To do this it uses our Metadata and Converters from `mojap_metadata`. It also uses `arrow_pd_parser` to ensure metadata conformance of different datatypes into Arrow/Pandas datatypes and then pushing this data into a database table with S3 and Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "from pyarrow import fs, parquet as pq\n",
    "from arrow_pd_parser import reader, writer\n",
    "from mojap_metadata import Metadata\n",
    "from mojap_metadata.converters.glue_converter import GlueConverter\n",
    "from mojap_metadata.converters.arrow_converter import ArrowConverter\n",
    "# from dataengineeringutils3 import s3\n",
    "import os\n",
    "import boto3\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "1. Create a test folder to work from (need access to `alpha-everyone`)\n",
    "2. Move that single dataset to S3\n",
    "3. Define the datas metadata using our Metadata class\n",
    "4. Read that data back from S3 using pd_arrow ensuring it conforms to our metadata\n",
    "5. Write the data to S3 into a database folder creating a csv table, jsonl table and parquet table (using awswrangler)\n",
    "6. Create the Table DDLs using the glueConverter and awswrangler\n",
    "7. Use aws wrangler to query each table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Add some key parameters probably the only thing you will need to change is your foldername. Second cell does some clean up using `awswrangler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup your own testing area (set foldername = GH username)\n",
    "foldername = \"isichei\" # GH username\n",
    "foldername = foldername.lower().replace(\"-\",\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"eu-west-1\"\n",
    "bucketname = \"alpha-everyone\"\n",
    "db_name = f\"aws_example_{foldername}\"\n",
    "db_base_path = f\"s3://{bucketname}/{foldername}/database\"\n",
    "s3_base_path = f\"s3://{bucketname}/{foldername}/\"\n",
    "if wr.s3.list_objects(s3_base_path):\n",
    "    print(\"deleting objs\")\n",
    "    wr.s3.delete_objects(s3_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Upload a dataset to S3\n",
    "\n",
    "Using `boto3`. There are many ways to do each of these. Hopefully this tutorial gives you a mix of how to do ones that we trust the most or have simple functions that others a re missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "with open(\"data/init-data.csv\", \"rb\") as f:\n",
    "    s3_client.upload_fileobj(f, bucketname, os.path.join(foldername, \"init-data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the data we just uploaded in S3 note we are reading the local version\n",
    "# pd.read_json(\"data/init-data.jsonl\", lines=True, dtype=str).head()\n",
    "pd.read_csv(\"data/init-data.csv\", dtype=str).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the metadata\n",
    "\n",
    "Create the metadata for the data using `mojap-metadata`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Define the metadata\n",
    "meta_dict = {\n",
    "    \"name\": \"test\",\n",
    "    \"description\": \"Some test data of different types\",\n",
    "    \"file_format\": \"jsonl\",\n",
    "    \"columns\":[\n",
    "        {\n",
    "            \"name\": \"character_col\",\n",
    "             \"type\": \"string\",\n",
    "            \"description\": \"This col has a tricky comma that messes with the serdes (parser). You'll see how we account for this later.\"\n",
    "        },\n",
    "        {\"name\": \"int_col\", \"type\": \"int32\"},\n",
    "        {\"name\": \"long_col\", \"type\": \"int64\"},\n",
    "        {\"name\": \"date_col\", \"type\": \"date64\"},\n",
    "        {\"name\": \"datetime_col\", \"type\": \"timestamp(s)\"},\n",
    "        {\"name\": \"boolean_col\", \"type\": \"bool_\"},\n",
    "        {\"name\": \"float_col\", \"type\": \"float32\"},\n",
    "        {\"name\": \"double_col\", \"type\": \"float64\"},\n",
    "        {\"name\": \"decimal_col\", \"type\": \"decimal128(5,3)\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "meta = Metadata.from_dict(meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conform the data to pandas\n",
    "\n",
    "Generate an arrow schema from our metadata (TODO) and then use that to ensure conformance with pandas (using `arrow_pd_parser`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_data_s3_path = os.path.join(\"s3://\", bucketname, foldername, \"init-data.csv\")\n",
    "\n",
    "# Read the data in from S3 this time\n",
    "\n",
    "df = reader.read(init_data_s3_path, metadata=meta)\n",
    "\n",
    "print(df.dtypes)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worth noting the type of decimal_col:\n",
    "type(df[\"decimal_col\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write the data to a database\n",
    "\n",
    "Can do this multiple ways so going to do it multiple ways.\n",
    "\n",
    "- (a) Going to write the data to a table using `awswrangler`\n",
    "- (b) Will write the data directly to S3 and use mojap-metadata and boto3 to create the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For both gonna need to do some setup\n",
    "database_path = os.path.join(bucketname, foldername, \"database\")\n",
    "\n",
    "databases = wr.catalog.databases()\n",
    "if db_name not in databases.values:\n",
    "    wr.catalog.create_database(db_name)\n",
    "    print(f\"Database '{db_name}' already exists\")\n",
    "else:\n",
    "    print(f\"Database '{db_name}' already exists\")\n",
    "\n",
    "for t in [\"csv_wr\", \"csv_gc\", \"csv_header\", \"csv_open\", \"jsonl_hive\", \"jsonl_openx\", \"parquet_table\"]:\n",
    "    if wr.catalog.delete_table_if_exists(database=db_name, table=t):\n",
    "        print(\"Deleted {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do (a)\n",
    "df_copy = df.copy() # wrangler castsa the col datatypes so gotta be careful as it changes df\n",
    "table_path = f\"s3://{database_path}/csv_wr\"\n",
    "wr.s3.to_csv(\n",
    "    df=df_copy,\n",
    "    path=table_path,\n",
    "    index=False,\n",
    "    dataset=True, # True allows the other params below i.e. overwriting to db.table\n",
    "    database=db_name,\n",
    "    table='csv_wr',\n",
    "    mode=\"overwrite\",\n",
    ")\n",
    "wr.athena.read_sql_query(f\"SELECT * FROM {db_name}.csv_wr\", database=db_name, ctas_approach=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now checkout the datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.catalog.get_table_types(database=db_name, table=\"csv_wr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the types here have been inferred by `awswranger` because we've lost some information between the conversion between arrow -> pandas (we know that our int_col is `int32` rather than `int64`).\n",
    "\n",
    "> When I originally wrote this tutorial with `aws-wrangler 1.10.1` It couldn't identify decimal types but now it does (now being 2.3.0) so probably in quick time they will identify other bits (although I dno why you would ever really want to choose 32 bit integer rather than 64 but anyway)...\n",
    "\n",
    "`awswrangler` is correctly and safely converting your pandas types to glue types to ensure they can be read safely in glue. We can use our metadata and converter to get the column types we exactly want. So let's do that as well..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5(b)\n",
    "\n",
    "So let's move on to using our converter to be exact about the datatypes (let's say we want to try and conform to their origin (as defined in the metadata). Let's write the data somewhere else to compare specifying a glue schema specific to our metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how we have to write out our CSV to work with the lazy hive schema\n",
    "# wrangler does this under the hood\n",
    "# Also worth noting that this is an aweful way to write CSVs do not use (quoting=csv.QUOTE_NONE) these parameters for any other CSV reader like R or Python!\n",
    "table_path = f\"s3://{database_path}/csv_gc\"\n",
    "wr.s3.to_csv(df, f\"{table_path}/csv_gc.csv\", index=False, header=False, escapechar=\"\\\\\", quoting=csv.QUOTE_NONE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a glue converter and convert our metadata to the GlueAPI specs\n",
    "gc = GlueConverter()\n",
    "meta.file_format = \"csv\"\n",
    "meta.name = \"csv_gc\"\n",
    "\n",
    "boto_dict = gc.generate_from_meta(meta, database_name=db_name, table_location=table_path)\n",
    "boto_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client(\"glue\")\n",
    "\n",
    "try:\n",
    "    _ = glue_client.delete_table(\n",
    "        DatabaseName=db_name,\n",
    "        Name=\"csv_gc\"\n",
    "    )\n",
    "except glue_client.exceptions.EntityNotFoundException:\n",
    "    print(\"table already deleted\")\n",
    "\n",
    "glue_client.create_table(**boto_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You'll see we get the same resulted output but we get more exact column types.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.athena.read_sql_query(f\"SELECT * FROM {db_name}.csv_gc\", database=db_name, ctas_approach=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.catalog.get_table_types(database=db_name, table=\"csv_gc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Let's make hella tables with the glueConverter\n",
    "\n",
    "We'll create the following:\n",
    "- CSV with header\n",
    "- CSV openSerde aweful for dates (requires UNIX epochs)\n",
    "- Jsonl (with Hive serde)\n",
    "- Jsonl (with Openx serde)\n",
    "- Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV with header\n",
    "\n",
    "We are actually going to do two things. We are going to specify the seperator as a pipe and write with header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.name = \"csv_header\"\n",
    "meta.file_format = \"csv\"\n",
    "\n",
    "# let's use the same glueConverter\n",
    "gc.options.csv.sep = \"|\"\n",
    "gc.options.csv.skip_header = True\n",
    "gc.options.default_db_name = db_name # set db name so we don't have to set it everytime\n",
    "gc.options.default_db_base_path = f\"s3://{database_path}/\" # set db name so we don't have to set it everytime\n",
    "full_table_path = f\"s3://{database_path}/{meta.name}/{meta.name}.csv\"\n",
    "wr.s3.to_csv(\n",
    "    df,\n",
    "    full_table_path,\n",
    "    index=False,\n",
    "    header=gc.options.csv.skip_header,\n",
    "    escapechar=gc.options.csv.escape_char,\n",
    "    sep=gc.options.csv.sep,\n",
    "    quoting=csv.QUOTE_NONE\n",
    ") # note header=True and sep=\"|\" escapechar is the same as defailt options for converter are \"\\\\\"\n",
    "\n",
    "spec = gc.generate_from_meta(meta)\n",
    "glue_client.create_table(**spec)\n",
    "wr.athena.read_sql_query(f\"SELECT * FROM {db_name}.{meta.name}\", database=db_name, ctas_approach=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV with openSerde\n",
    "\n",
    "We can use a different serde (aka parser) for CSVs. The pros of this parser means that having commas in the character cols work when you write out CSVs like a normal person (quoted values). However, the con here is that it requires you to write out dates and timestamps are based on unix timesstamps. [More on this con](https://stackoverflow.com/questions/52564194/athena-unable-to-parse-date-using-opencsvserde). The TLDR is that dates should be integer days from 1 January 1970 and timestamps should an integer in milliseconds that have elapsed since Midnight 1 January 1970.\n",
    "\n",
    "> This actually fails will also fail unless your data is not NULL - so just avoid openCSV to be honest. More on this null value error: https://aws.amazon.com/premiumsupport/knowledge-center/athena-hive-bad-data-error-csv/\n",
    "So in this example we treat everything but the date and datetimes (which I also fill as 0 just to demonstrate the date types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_open_csv = Metadata.from_dict(meta_dict)\n",
    "meta_open_csv.name = \"csv_open\"\n",
    "meta_open_csv.file_format = \"csv\"\n",
    "\n",
    "# Do unix conversion\n",
    "df_unix = df.copy()\n",
    "\n",
    "epoch = datetime.datetime.utcfromtimestamp(0)\n",
    "\n",
    "df_unix.datetime_col = df_unix.datetime_col.apply(lambda x: 0 if pd.isna(x) else int((x - epoch).total_seconds() * 1000.0))\n",
    "df_unix.date_col = df_unix.date_col.apply(lambda x: 0 if pd.isna(x) else (x -  datetime.date(1970, 1, 1)).days)\n",
    "\n",
    "# ALSO Cant do decimal\n",
    "for c in meta_open_csv.columns:\n",
    "    if c[\"type\"].startswith(\"decimal\"):\n",
    "        c[\"type\"] = \"float64\"\n",
    "    elif not c[\"name\"].startswith(\"date\"):\n",
    "        c[\"type\"] = \"string\"\n",
    "\n",
    "# let's use the same glueConverter\n",
    "gc.options.set_csv_serde(\"open\") # openCSVSerde\n",
    "gc.options.csv.sep = \",\"\n",
    "gc.options.csv.skip_header = False\n",
    "\n",
    "full_table_path = f\"s3://{database_path}/{meta_open_csv.name}/{meta_open_csv.name}.csv\"\n",
    "wr.s3.to_csv(\n",
    "    df_unix,\n",
    "    full_table_path,\n",
    "    index=False,\n",
    "    header=gc.options.csv.skip_header,\n",
    "    sep=gc.options.csv.sep\n",
    ") # note we are using quotes again and a comma sep!\n",
    "\n",
    "spec = gc.generate_from_meta(meta_open_csv)\n",
    "glue_client.create_table(**spec)\n",
    "wr.athena.read_sql_query(f\"SELECT * FROM {db_name}.{meta_open_csv.name}\", database=db_name, ctas_approach=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝ What a palaver - don't do this unless you definitely want this serde because you hate yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jsonl with Hive Serde\n",
    "\n",
    "JSONL files whoop whoop. Hive serde is the default for the glue-converter. Pandas writer is still not quite there for jsonl so you should cast your dates and datetimes yourself into the appropriate ISO format. Also worth noting that currently awswrangler doesn't have an option to add a pandas table to glue for jsonl types (it only supports CSV and parquet). Most likely this will change in the near future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast dates and timestamps to str\n",
    "df_json = df.copy()\n",
    "df_json.date_col = df_json.date_col.apply(lambda x: pd.NA if pd.isna(x) else str(x)).astype(\"string\")\n",
    "df_json.datetime_col = df_json.datetime_col.apply(lambda x: pd.NA if pd.isna(x) else str(x)).astype(\"string\")\n",
    "\n",
    "meta.name = \"jsonl_hive\"\n",
    "meta.file_format = \"jsonl\" # can also write json\n",
    "\n",
    "gc.options.set_json_serde(\"hive\")\n",
    "# let's use the same glueConverter\n",
    "full_table_path = f\"s3://{database_path}/{meta.name}/{meta.name}.jsonl\"\n",
    "wr.s3.to_json(\n",
    "    df_json,\n",
    "    full_table_path,\n",
    "    orient=\"records\",\n",
    "    lines=True\n",
    ") # note header=True and sep=\"|\" escapechar is the same as defailt options for converter are \"\\\\\"\n",
    "\n",
    "spec = gc.generate_from_meta(meta)\n",
    "glue_client.create_table(**spec)\n",
    "wr.athena.read_sql_query(f\"SELECT * FROM {db_name}.{meta.name}\", database=db_name, ctas_approach=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.catalog.get_table_types(database=db_name, table=meta.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jsonl with Openx Serde\n",
    "\n",
    "Now the openx serde. Differences between this and the Hive one can be [found here](https://docs.aws.amazon.com/athena/latest/ug/json-serde.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.name = \"jsonl_openx\"\n",
    "meta.file_format = \"jsonl\" # can also write json\n",
    "\n",
    "gc.options.set_json_serde(\"openx\")\n",
    "# let's use the same glueConverter\n",
    "full_table_path = f\"s3://{database_path}/{meta.name}/{meta.name}.jsonl\"\n",
    "wr.s3.to_json(\n",
    "    df_json,\n",
    "    full_table_path,\n",
    "    orient=\"records\",\n",
    "    lines=True\n",
    ") # note header=True and sep=\"|\" escapechar is the same as defailt options for converter are \"\\\\\"\n",
    "\n",
    "spec = gc.generate_from_meta(meta)\n",
    "glue_client.create_table(**spec)\n",
    "wr.athena.read_sql_query(f\"SELECT * FROM {db_name}.{meta.name}\", database=db_name, ctas_approach=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.catalog.get_table_types(database=db_name, table=meta.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet\n",
    "\n",
    "Finally! This is the easiest dataset to work with because it has metadata built into the data (that does mean you cannot eye ball it with a text editor but who cares). Strongly recommend using this type because it works well with arrow and glue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our meta changes again\n",
    "meta.name = \"parquet_table\"\n",
    "meta.file_format = \"parquet\"\n",
    "\n",
    "full_table_path = f\"s3://{database_path}/{meta.name}/{meta.name}.snappy.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now instead of writing to S3 using wrangler we are going to use arrow instead. This allows us to be super specific about our metadata conformance which is why we are all here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe to an arrow table and then cast to our specific metadata then write to S3\n",
    "table = pa.Table.from_pandas(df)\n",
    "table = table.cast(arrow_schema)\n",
    "\n",
    "# Write the data to S3 and then generate the glue table as normal\n",
    "s3 = fs.S3FileSystem(region='eu-west-1')\n",
    "with s3.open_output_stream(full_table_path.replace(\"s3://\",\"\")) as f:\n",
    "    pq.write_table(table, f)\n",
    "\n",
    "spec = gc.generate_from_meta(meta)\n",
    "glue_client.create_table(**spec)\n",
    "wr.athena.read_sql_query(f\"SELECT * FROM {db_name}.{meta.name}\", database=db_name, ctas_approach=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.catalog.get_table_types(database=db_name, table=meta.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mojap-demo",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
